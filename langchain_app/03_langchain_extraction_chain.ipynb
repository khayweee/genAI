{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "996c2c98",
   "metadata": {},
   "source": [
    "# Build and Extraction Chain\n",
    "Using `tool-calling` features of `chat models` to extract structured information from unstructured text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36340f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfdf39e",
   "metadata": {},
   "source": [
    "### Best practices in defining schema\n",
    "There are two best practices when defining schema:\n",
    "\n",
    "Document the attributes and the schema itself: This information is sent to the LLM and is used to improve the quality of information extraction.\n",
    "Do not force the LLM to make up information! Above we used Optional for the attributes allowing the LLM to output None if it doesn't know the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1550b379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define what information we want to extract from the text\n",
    "class Person(BaseModel):\n",
    "    \"\"\" Information about a person. \"\"\"\n",
    "    name: Optional[str] = Field(default=None, description=\"Name of the person\")\n",
    "    hair_color: Optional[str] = Field(default=None, description=\"Hair color of the person\")\n",
    "    height_in_meters: Optional[str] = Field(default=None, description=\"Height of the person in meters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4f862ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom prompt to provide instructions and any additional context\n",
    "# 1) You can add examples into the prompt template to improve extraction quality\n",
    "# 2) Introduce additional parameters to take context into account (e.g., Include metadata about the document from which the text was extracted.)\n",
    "\n",
    "# here we use .from_messages because it allows us to define a multi-role, multi-turn conversation\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', 'You are an expert extraction algorithm. '\n",
    "        \"Only extract relevant information from text.\"\n",
    "        \" If you do not know the value of an attribute asked to extract,\"\n",
    "        \" return null for that attribute's value.\"\n",
    "        ),\n",
    "        (\"human\", \"{text}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a45e4bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPERATURE = 0.0\n",
    "NUM_PREDICT = 256\n",
    "MODEL = \"gemma3:12b-it-qat\"\n",
    "\n",
    "llm = init_chat_model(\n",
    "    model=MODEL,\n",
    "    temperature=TEMPERATURE,\n",
    "    num_predict=NUM_PREDICT,\n",
    "    use_gpu=True,\n",
    "    model_provider='ollama',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11a75dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_llm = llm.with_structured_output(schema=Person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a277eef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Alan Smith is 6 feet tall and has blond hair.\"\n",
    "prompt = prompt_template.invoke({\"text\": text})\n",
    "structured_output = structured_llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abd538a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: messages=[SystemMessage(content=\"You are an expert extraction algorithm. Only extract relevant information from text. If you do not know the value of an attribute asked to extract, return null for that attribute's value.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Alan Smith is 6 feet tall and has blond hair.', additional_kwargs={}, response_metadata={})]\n",
      "Structured Output: name='Alan Smith' hair_color=None height_in_meters=None\n"
     ]
    }
   ],
   "source": [
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Structured Output: {structured_output}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f222ea8",
   "metadata": {},
   "source": [
    "## Multiple Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a54d9b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(BaseModel):\n",
    "    \"\"\" Extracted data about people \"\"\"\n",
    "    people: List[Person]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e10a0d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people=[Person(name='Jeff', hair_color='black', height_in_meters=None), Person(name='Anna', hair_color='black', height_in_meters=None)]\n"
     ]
    }
   ],
   "source": [
    "structured_llm = llm.with_structured_output(schema=Data)\n",
    "text = \"My name is Jeff, my hair is black and i am 6 feet tall. Anna has the same color hair as me.\"\n",
    "prompt = prompt_template.invoke({\"text\": text})\n",
    "structured_output = structured_llm.invoke(prompt)\n",
    "print(structured_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabaab0f",
   "metadata": {},
   "source": [
    "## Tools Reference\n",
    "`Structured output` often uses tool calling under-the-hood. This typically involves the generation of `AI Messages` containing tool calls, as well as `tool messages` containing the results of tool calls.  \n",
    "\n",
    "Here we are demonstrating how does providing a **few shot** learning examples help to direct the LLM to learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f3f0dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dm/2l6n86d53q51l41z3nqv0tw40000gp/T/ipykernel_67419/2059799454.py:21: LangChainBetaWarning: The function `tool_example_to_messages` is in beta. It is actively being worked on, so the API may change.\n",
      "  tool_example_to_messages(\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.utils.function_calling import tool_example_to_messages\n",
    "\n",
    "examples = [\n",
    "    (\n",
    "        \"The ocean is vast and blue. It is more than 20,000 feet deep.\",\n",
    "        Data(people=[])\n",
    "    ),\n",
    "    (\n",
    "        'Fiona traveled far from France to Spain.',\n",
    "        Data(people=[Person(name=\"Fiona\", hair_color=None, height_in_meters=None)]),\n",
    "    )\n",
    "]\n",
    "\n",
    "messages = []\n",
    "for txt, tool_call in examples:\n",
    "    if tool_call.people:\n",
    "        ai_response = \"Detected People.\"\n",
    "    else:\n",
    "        ai_response = \"Detected no people.\"\n",
    "    messages.extend(\n",
    "        tool_example_to_messages(\n",
    "            txt,\n",
    "            [tool_call],\n",
    "            ai_response=ai_response,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2e99dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "The ocean is vast and blue. It is more than 20,000 feet deep.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  Data (e00aeb52-bd1d-4a73-ac46-ea26a0b37369)\n",
      " Call ID: e00aeb52-bd1d-4a73-ac46-ea26a0b37369\n",
      "  Args:\n",
      "    people: []\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "You have correctly called this tool.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Detected no people.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Fiona traveled far from France to Spain.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  Data (563da6ee-6209-4f71-af69-b9e857884895)\n",
      " Call ID: 563da6ee-6209-4f71-af69-b9e857884895\n",
      "  Args:\n",
      "    people: [{'name': 'Fiona', 'hair_color': None, 'height_in_meters': None}]\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "You have correctly called this tool.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Detected People.\n"
     ]
    }
   ],
   "source": [
    "for message in messages:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd21352e",
   "metadata": {},
   "source": [
    "### Comparing performance with and without these messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "593be3a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(people=[Person(name='Neil Armstrong', hair_color=None, height_in_meters=None), Person(name='Buzz Aldrin', hair_color=None, height_in_meters=None)])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_no_extraction = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"The solar system is large, but earth has only 1 moon which has Neil Armstrong's Foot imprint.\",\n",
    "}\n",
    "\n",
    "structured_llm = llm.with_structured_output(schema=Data)\n",
    "structured_llm.invoke([message_no_extraction])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "501a29c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(people=[Person(name='Neil Armstrong', hair_color=None, height_in_meters=None)])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_llm.invoke(messages + [message_no_extraction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e0e3c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
