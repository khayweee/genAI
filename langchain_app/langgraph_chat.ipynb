{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff15ad5c",
   "metadata": {},
   "source": [
    "# Building A chatbot with LangGraph\n",
    "LangGraph offers Message Persistence through a built-in persistence layer.\n",
    "- Ideal for chat applications that support multiple conversational turns\n",
    "\n",
    "Wrapping our chat model in a minimal LangGraph application allows us to automatically persist the message history, simplifying the development of multi-turn applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f59bb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain.chat_models import init_chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60d959ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPERATURE = 0.0\n",
    "NUM_PREDICT = 256\n",
    "MODEL = \"gemma3:12b-it-qat\"\n",
    "\n",
    "model = init_chat_model(\n",
    "    model=MODEL,\n",
    "    temperature=TEMPERATURE,\n",
    "    num_predict=NUM_PREDICT,\n",
    "    use_gpu=True,\n",
    "    model_provider='ollama',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d08ae3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new Graph\n",
    "workflow = StateGraph(state_schema=MessagesState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fa14de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state['messages'])\n",
    "    return {\"messages\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37aff27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the single node in the graph\n",
    "workflow.add_node(\"model\", call_model)\n",
    "workflow.add_edge(START, \"model\")\n",
    "\n",
    "# Add Memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fd50f8",
   "metadata": {},
   "source": [
    "We now need to create a config that we pass into the runnable every time. This config contains information that is not part of the input directly, but is still useful. In this case, we want to include a thread_id. This should look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbb29c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "123d6401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Bob! It's nice to meet you. I'm a large language model. What can I do for you today?\n"
     ]
    }
   ],
   "source": [
    "query = \"Hi! I'm Bob.\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()  # output contains all messages in state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11bf1713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Bob! You told me that at the beginning of our conversation. ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "query = \"What is my name.\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()  # output contains all messages in state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b598b1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi! I'm Bob.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Bob! It's nice to meet you. I'm a large language model. What can I do for you today?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is my name.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Bob! You told me that at the beginning of our conversation. ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "for text in output['messages']:\n",
    "    text.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caac673c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
